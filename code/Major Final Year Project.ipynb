{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import random\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image,ImageOps\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D , MaxPooling2D, Flatten, Dense , Dropout\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe():\n",
    "    path = 'rawframes'\n",
    "    row = 64\n",
    "    col = 64\n",
    "    column_names = ['pixel'+str(i) for i in range(row*col)]\n",
    "    column_names_with_label = column_names.copy()\n",
    "    column_names_with_label.append('label')\n",
    "    numclass=0\n",
    "    numfol=0\n",
    "    files=0\n",
    "    filec=0\n",
    "    twod = []\n",
    "    count = 0\n",
    "\n",
    "    for class_name in os.listdir(path):\n",
    "        classcount=0\n",
    "        if(classcount==51):break\n",
    "        if(class_name!='.DS_Store'):\n",
    "            numfol=0\n",
    "            for fol in os.listdir(path+'/'+class_name):\n",
    "                if(fol[-9:]!='.DS_Store'):\n",
    "                    numfol+=1\n",
    "                    if(numfol==30):break\n",
    "                    numfil=0\n",
    "                    for filename in os.listdir(path+'/'+class_name+'/'+fol):\n",
    "                        numfil+=1\n",
    "                        if(filename[-9:] != '.DS_Store' and numfil%10==0):\n",
    "                            \n",
    "                            img = Image.open(path+'/'+class_name+'/'+fol+'/'+filename).convert('L')\n",
    "                            img = img.resize((col,row))\n",
    "                            img = np.array(img).flatten()\n",
    "                                \n",
    "                            dict_ = {column_names[i]: img[i] for i in range(len(column_names))}\n",
    "                            dict_['label'] = class_name\n",
    "                            twod.append(list(dict_.values()))\n",
    "                            count+=1\n",
    "                            if(count == 30000): break\n",
    "                        \n",
    "                            print(count)          \n",
    "           \n",
    "    df = pd.DataFrame(twod,columns=column_names_with_label)\n",
    "    df.to_csv('data.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_data():\n",
    "    data = pd.read_csv('data.csv')\n",
    "    hogarr=[]\n",
    "    \n",
    "    for index,row in data.iterrows():\n",
    "        img = np.reshape(np.array(row[:-1],dtype=int),(64,64)).astype('uint8')\n",
    "        hog_feature, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2),block_norm= 'L2', visualize=True, multichannel=False)\n",
    "        hogarr.append(list(hog_feature))    \n",
    "    hogdf = pd.DataFrame(hogarr, columns=['pixel'+str(i) for i in range(len(hog_feature))])\n",
    "    \n",
    "    extra_column = data['label']\n",
    "    hogdf = pd.concat([hogdf,extra_column], axis = 1)\n",
    "   \n",
    "    hogdf.to_csv('HOGdata.csv',index=None)\n",
    "  \n",
    "    hog_y = pd.DataFrame(hogdf , columns =['label'])\n",
    "   \n",
    "    hog_le = LabelEncoder()\n",
    "    hog_y = hog_le.fit_transform(hog_y)\n",
    "    \n",
    "    hog_train =hogdf\n",
    "    hog_train = hog_train.drop('label', axis=1).values\n",
    "     \n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    \n",
    "    print('hog extraction completed .... Working on classification')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(hog_train, hog_y, test_size=0.2, shuffle = True)\n",
    "    \n",
    "    \n",
    "        # *************************** SVM + HOG *********************************\n",
    "    normalized_x_train_svm = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "    normalized_x_test_svm = pd.DataFrame(scaler.fit_transform(X_test))\n",
    "   \n",
    "    svm = LinearSVC()\n",
    "    svm.fit(normalized_x_train_svm,y_train)\n",
    "    svm_preds = svm.predict(normalized_x_test_svm)\n",
    "    svm_acc_hog = accuracy_score(y_test,svm_preds)\n",
    "    \n",
    "    \n",
    "    # ************************************ HOG + DNN ********************************\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    XX_train = X_train[:,0:].reshape(X_train.shape[0],X_train[1].size).astype('float32')\n",
    "        \n",
    "    XX_train = XX_train / 255.0\n",
    "    \n",
    "    \n",
    "    XX_test = X_test[:,0:].reshape(X_test.shape[0],X_test[1].size).astype('float32')\n",
    "    XX_test = XX_test / 255.0\n",
    "    \n",
    "    \n",
    "    classifier = Sequential()\n",
    "  \n",
    "    classifier.add(Dense(output_dim = 512, input_shape=(1764,)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.4))    \n",
    "\n",
    "\n",
    "    classifier.add(Dense(output_dim = 256))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.4))    \n",
    "    \n",
    "\n",
    "    classifier.add(Dense(output_dim = 128))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.4))    \n",
    "    \n",
    "    # classifier.add(Flatten())\n",
    "    classifier.add(Dense(output_dim = 51, activation= 'softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    classifier.summary()\n",
    "    \n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)\n",
    "    early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "   \n",
    "    \n",
    "   \n",
    "    history = classifier.fit(XX_train,y_train,\n",
    "                    validation_data= (XX_test,y_test),\n",
    "                    batch_size=32, epochs = 100 ,\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_reducer, early_stopper ])\n",
    "    \n",
    "    classifier.save_weights('hog_and_cnn.h5')\n",
    "    \n",
    "    acc_hog = history.history['acc']\n",
    "    val_acc_hog = history.history['val_acc']\n",
    "    loss_hog = history.history['loss']\n",
    "    val_loss_hog = history.history['val_loss']\n",
    "    \n",
    "    epochs_hog = range(len(acc_hog))    \n",
    "\n",
    "    return(epochs_hog, acc_hog, loss_hog, val_acc_hog, val_loss_hog,svm_acc_hog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT_data():\n",
    "    data = pd.read_csv('data.csv')\n",
    "    siftarr=[]\n",
    "    mindescriptors=1000\n",
    "    eps=1e-7\n",
    "\n",
    "    for index,row in data.iterrows():\n",
    "        img = np.reshape(np.array(row[:-1],dtype=int),(64,64)).astype('uint8')\n",
    "        sift = cv2.xfeatures2d.SURF_create(10)\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        if(descriptors is not None):\n",
    "            mindescriptors = min(mindescriptors,descriptors.shape[0])\n",
    "\n",
    "    for index,row in data.iterrows():\n",
    "        img = np.reshape(np.array(row[:-1],dtype=int),(64,64)).astype('uint8')\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures= mindescriptors,\n",
    "                            nOctaveLayers=3,\n",
    "                            contrastThreshold=0.04,\n",
    "                            edgeThreshold=10.0,\n",
    "                            sigma=1.6) #nfeatures=mindescriptors)\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        \n",
    "        if(descriptors is not None):\n",
    "            descriptors=descriptors[:mindescriptors]\n",
    "            sift_feature = np.array(descriptors).flatten()\n",
    "            siftarr.append(list(sift_feature))\n",
    "\n",
    "    siftdf = pd.DataFrame(siftarr,columns=['pixel'+str(i) for i in range(len(sift_feature))])\n",
    "    \n",
    "    extra_column = data['label']\n",
    "    siftdf = pd.concat([siftdf,extra_column], axis = 1)\n",
    "    siftdf.fillna(siftdf.median(), inplace=True)\n",
    "    \n",
    "    siftdf.to_csv('SIFTdata.csv',index=None)\n",
    "    \n",
    "    sift_y = pd.DataFrame(siftdf , columns =['label'])\n",
    "   \n",
    "    sift_le = LabelEncoder()\n",
    "    sift_y = sift_le.fit_transform(sift_y)\n",
    "    \n",
    "    sift_train =siftdf\n",
    "    sift_train = sift_train.drop('label', axis=1).values\n",
    "        \n",
    "    \n",
    "     # --------------------------\n",
    "    print('SIFT extraction completed .... Working on CNN')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(sift_train, sift_y, test_size=0.2, shuffle = True)\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    XX_train = X_train[:,0:].reshape(X_train.shape[0],128).astype('float32')\n",
    "        \n",
    "    XX_train = XX_train / 255.0\n",
    "    \n",
    "    \n",
    "    XX_test = X_test[:,0:].reshape(X_test.shape[0],128).astype('float32')\n",
    "    XX_test = XX_test / 255.0\n",
    "    \n",
    "    \n",
    "    classifier = Sequential()\n",
    "    \n",
    "    #  classifier.add(Flatten())\n",
    "    classifier.add(Dense(output_dim = 512, input_shape=(128,)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.2))    \n",
    "\n",
    "    classifier.add(Dense(output_dim = 256))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.2))    \n",
    "\n",
    "\n",
    "    classifier.add(Dense(output_dim = 128))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.2))    \n",
    "    \n",
    "    # classifier.add(Flatten())\n",
    "    classifier.add(Dense(output_dim = 51, activation= 'softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    classifier.summary()\n",
    "    \n",
    "    \n",
    "    #filename='my_model_sift_cnn_data.csv'\n",
    "    #filepath=\"my_model_SIFT_CNN_Best-weights-my_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5\"\n",
    "\n",
    "    #csv_log=callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "    #checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    #callbacks_list = [csv_log,checkpoint]\n",
    "    #callbacks_list = [csv_log]\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)\n",
    "    early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "   \n",
    "    \n",
    "    \n",
    "    history = classifier.fit(XX_train,y_train,\n",
    "                    validation_data= (XX_test,y_test),\n",
    "                    batch_size=32, epochs = 100 ,\n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_reducer, early_stopper])\n",
    "   \n",
    "    classifier.save_weights('sift_and_cnn.h5')\n",
    "\n",
    "    acc_sift = history.history['acc']\n",
    "    val_acc_sift = history.history['val_acc']\n",
    "    loss_sift = history.history['loss']\n",
    "    val_loss_sift = history.history['val_loss']\n",
    "    \n",
    "    epochs_sift = range(len(acc_sift))\n",
    "\n",
    "    return (epochs_sift,acc_sift,loss_sift, val_acc_sift,val_loss_sift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data():\n",
    "    # Importing the dataset\n",
    "    dataset = pd.read_csv('data.csv')\n",
    "    data_encoded = dataset\n",
    "    row = len(dataset)\n",
    "    col = len(dataset.columns)\n",
    "\n",
    "    number_classes=  data_encoded['label'].value_counts()\n",
    "    \n",
    "\n",
    "    le = LabelEncoder()\n",
    "    data_encoded['label'] = le.fit_transform(data_encoded['label'])\n",
    "    X = dataset.iloc[:, 0:col-1].values\n",
    "    y = dataset.iloc[:, col-1].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0, shuffle=True)\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    XX_train = X_train[:,0:].reshape(X_train.shape[0],64,64,1).astype('float32')\n",
    "        \n",
    "    XX_train = XX_train / 255.0\n",
    "    \n",
    "    \n",
    "    XX_test = X_test[:,0:].reshape(X_test.shape[0],64,64,1).astype('float32')\n",
    "    XX_test = XX_test / 255.0\n",
    "    \n",
    "    \n",
    "    classifier = Sequential()\n",
    "    classifier.add(Convolution2D(64,(3,3),input_shape = (64,64,1)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Convolution2D(64,(3,3),input_shape = (64,64,1)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    classifier.add(Dropout(0.4))    \n",
    "   \n",
    "    classifier.add(Convolution2D(128,(3,3)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Convolution2D(128,(3,3)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    classifier.add(Dropout(0.4))    \n",
    "    \n",
    "    classifier.add(Convolution2D(256,(3,3)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Convolution2D(128,(3,3)))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    classifier.add(Dropout(0.4))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(output_dim = 256))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.4))    \n",
    "\n",
    "    classifier.add(Dense(output_dim = 128))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(0.4))    \n",
    "    \n",
    "    \n",
    "    classifier.add(Dense(output_dim = number_classes.shape[0] , activation= 'softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    classifier.summary()\n",
    "    \n",
    " \n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)\n",
    "    early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "    \n",
    "    history= classifier.fit(XX_train,y_train,\n",
    "                   validation_data = (XX_test,y_test),\n",
    "                   batch_size=32,\n",
    "                   epochs = 100 , \n",
    "                   verbose=1,\n",
    "                   callbacks=[lr_reducer, early_stopper])\n",
    "\n",
    "    classifier.save_weights('only_cnn.h5')\n",
    "    \n",
    "    acc_cnn = history.history['acc']\n",
    "    val_acc_cnn = history.history['val_acc']\n",
    "    loss_cnn = history.history['loss']\n",
    "    val_loss_cnn = history.history['val_loss']\n",
    "    \n",
    "    epochs_cnn = range(len(acc_cnn))\n",
    "    \n",
    "    return(epochs_cnn,acc_cnn, loss_cnn, val_acc_cnn, val_loss_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****************************************************')\n",
    "print('making dataframe')\n",
    "make_dataframe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****************************************************')\n",
    "print('SIFT keypoints and descriptors extraction')\n",
    "epochs_sift,acc_sift,loss_sift, val_acc_sift,val_loss_sift = SIFT_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****************************************************')\n",
    "print('HOG feature extraction')\n",
    "epochs_hog, acc_hog, loss_hog, val_acc_hog, val_loss_ho, svm_acc_hog = hog_data()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****************************************************')\n",
    "print('CNN')\n",
    "epochs_cnn,acc_cnn, loss_cnn, val_acc_cnn, val_loss_cnn = cnn_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- SIFT + CNN ------------------------------------\n",
    "\n",
    "#print('training accuracy of SIFT-CNN: {:.2f} '.format(acc_sift*100) + '% ')\n",
    "#print('testing accuracy of SIFT-CNN:{:.2f} '.format(val_acc_sift*100) + '% ')\n",
    "\n",
    "plt.plot(epochs_sift,acc_sift , 'r' , label = \"Training accuracy\")\n",
    "plt.plot(epochs_sift,val_acc_sift , 'b' , label = \"Validation accuracy\")\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "    \n",
    "plt.plot(epochs_sift,loss_sift , 'r' , label = \"Training loss\")\n",
    "plt.plot(epochs_sift,val_loss_sift , 'b' , label = \"Validation loss\")\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- HOG + CNN ---------------------\n",
    "\n",
    "# ************** change the index at the last epoch the training ended in print statement .\n",
    "\n",
    "print('training accuracy of HOG-CNN:')\n",
    "#print((acc_hog[epochs_hog[23]]))\n",
    "print('validation accuracy of HOG-CNN:')\n",
    "#print((val_acc_hog[epochs_hog[23]]))\n",
    "\n",
    "print(\"accuracy of SVM HOG\".format(svm_acc_hog))\n",
    "\n",
    "plt.plot(epochs_hog,acc_hog , 'r' , label = \"Training accuracy\")\n",
    "plt.plot(epochs_hog,val_acc_hog , 'b' , label = \"Validation accuracy\")\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs_hog,loss_hog , 'r' , label = \"Training loss\")\n",
    "plt.plot(epochs_hog,val_loss_hog , 'b' , label = \"Validation loss\")\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- CNN -----------------------\n",
    "\n",
    "print('Training accuracy of CNN:{:.2f}'.format(acc_cnn*100) + '% ')\n",
    "print('Testing accuracy of CNN:{:.2f}'.format(val_acc_cnn*100) + '% ')\n",
    "        \n",
    "\n",
    "plt.plot(epochs_cnn,acc_cnn , 'r' , label = \"Training accuracy\")\n",
    "plt.plot(epochs_cnn,val_acc_cnn , 'b' , label = \"Validation accuracy\")\n",
    "plt.title('Training and Validation accuracy')\n",
    "    \n",
    "plt.plot(epochs_cnn,loss_cnn , 'r' , label = \"Training loss\")\n",
    "plt.plot(epochs_cnn,val_loss_cnn , 'b' , label = \"Validation loss\")\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
